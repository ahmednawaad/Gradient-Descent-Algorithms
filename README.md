# Gradient-Descent-Algorithms

Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.

## Here is my implementation of GD Algorithms
1- Batch GD

2- Mini-Batch 

3- Stochastic

4- Momentum 

5- NAG

6- Adagrad

7- RMSProp

8- Adam

##  If it was not working, these links are for colabs
https://drive.google.com/file/d/1qX4Kj-7HcyDlSKlixweGg7mxlpDcffgW/view?usp=sharing
https://drive.google.com/file/d/1CeHvgnSPpUpAMiIfJLL7aKAVBUbQGhOj/view?usp=sharing
https://drive.google.com/file/d/1WF1lY4VZPxMiJdgJewI_VGTqcp2RDPvx/view?usp=sharing

## here is my comparison between these algorithms

https://drive.google.com/file/d/14ilsKdUWm3NtHvpZt_fDobDB-JuTz7mf/view?usp=sharing
