# Gradient-Descent-Algorithms

Gradient Descent is an optimization algorithm for finding a local minimum of a differentiable function. Gradient descent is simply used to find the values of a function's parameters (coefficients) that minimize a cost function as far as possible.

## Here is my implementation of GD Algorithms
1- Batch GD

2- Mini-Batch 

3- Stochastic

4- Momentum 

5- NAG

6- Adagrad

7- RMSProp

8- Adam

## here is my comparison between these algorithms

https://drive.google.com/file/d/14ilsKdUWm3NtHvpZt_fDobDB-JuTz7mf/view?usp=sharing
